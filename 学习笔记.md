# 机器学习经典算法学习笔记

## 第一章 线性回归

- ### 1.1 原理

线性回归为**有监督算法（需要标签）**。

假设 $\theta_1$ 是 $x_1$ 的参数，$\theta_2$ 是 $x_2$ 的参数

拟合的平面： $h_\theta(x)=\theta_0+\theta_{1} x_1+\theta_2 x_2$ 

**整合**：
$$
h_\theta(x)=\sum_{i=0}^{n}\theta_i x_i=\theta^Tx
$$

**误差**：

真实值和预测值之间 存在的差异（$\varepsilon$)

对于每个样本：
$$
y^{(i)}=\theta^Tx^{(i)}+\varepsilon^{(i)}\tag{1}
$$
​                                                  

**误差 $\varepsilon^{(i)}$  是独立同分布，是服从均值为 0 方差为 $\theta^2$ 的高斯分布 **（机器学习的基础）

由于误差服从高斯分布：
$$
p(\varepsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\varepsilon^{(i)})^2}{2\sigma^2})\tag{2}
$$
将（1）式带入（2）式：
$$
p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
$$
求得 $(x,\theta)$ 与 $y$ 越接近越好！

**似然函数**：
$$
L(\theta) = \prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta) = \prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
$$
**对数似然**：
$$
log\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
$$
展开化简：
$$
\sum_{i=1}^{m}log\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\
= m\cdot log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2
$$
目标：是让似然函数越大越好

最小二乘法：
$$
J(\theta) = \frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2
$$
目标：让$J(\theta)$ 越小越好！（目标函数，损失函数 Loss Function）

**目标函数求解**

目标函数：
$$
J(\theta) = \frac{1}{2}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 = \frac{1}{2}(X\theta-y)^T(X\theta-y)
$$
求偏导：
$$
\bigtriangledown_\theta J(\theta)=\bigtriangledown_\theta(\frac{1}{2}(X\theta-y)^T(X\theta-y)\\ =\bigtriangledown_\theta(\frac{1}{2}(\theta^TX^T-y^T)(X\theta-y))\\ =\bigtriangledown_\theta(\frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty))\\ =\frac{1}{2}(2X^TX\theta-X^Ty-(y^TX)^T)=X^TX\theta-X^Ty
$$
偏导等于0：
$$
\theta = (X^TX)^{-1}X^Ty
$$

**梯度下降**

寻找目标函数的“终点”（什么样的参数能使目标函数达到极值点） 

目标函数：
$$
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(y^i-h_\theta(x^i))^2
$$
**批量梯度下降**：
$$
\frac{\partial J(\theta)}{\partial \theta_j} = -\frac{1}{m}\sum_{i=1}^{m}(y^i-h_\theta(x^i))x_j^i\theta_j^\prime = \theta_j+\frac{1}{m}\sum_{i=1}^{m}(y^i-h_\theta(x^i))x_j^i
$$
(容易得到最优解，但是由于每次考虑所有样本，速度很慢)

**随机梯度下降(SGD)**：
$$
\theta_j^\prime = \theta_j + (y^i-h_\theta(x^i))x_j^i
$$
（每次找一个样本，迭代速度快，但不一定每次都朝着收敛方向）

**小批量梯度下降法（mini batch）**：
$$
\theta_j := \theta_j - \alpha\frac{1}{10}\sum_{k=i}^{i+9}(h_\theta(x^{(k)})-y^{(k)})x_j^{(k)}
$$
(每次更新选择一小部分数据来算，实用！)

**学习率（步长）learning rate**：对结果产生巨大的影响，一般小一些

- ### 1.2 代码实现

- #### 1.2.1 模块编写

**归一化模块**

[normalize.py](LinearRegression/normalize.py)

计算特征（输入）的均值feature_mean，标准差feature_deviation。

归一化操作：
$$
features\_normalized = \frac{x-\mu}{\sigma}
$$
返回归一化后的特征，均值，标准差

**训练前要做的准备**

[prepare_for_traning.py](LinearRegression/prepare_for_training.py)

- 计算样本总数
- 预处理（调用normalize模块）
- 返回处理后的数据，均值，标准差

**定义线性回归类**

[linear_regression.py](LinearRegression/linear_regression.py)

- #### 1.2.2 单变量线性回归实现

[源码](LinearRegression/UnivariateLinearRegression.ipynb)

使用的数据为[world_happiness_report2017.csv](LinearRegression/data/world_happiness_report2017.csv)

**目的**：训练出GDP与幸福指数之间的线性回归方程

导入数据：

```python
data = pd.read_csv('data/world_happiness_report2017.csv')
train_data = data.sample(frac=0.8)
test_data = data.drop(train_data.index)
input_param_name = 'Economy..GDP.per.Capita.'
output_param_name = 'Happiness.Score'
x_train = train_data[[input_param_name]].values
y_train = train_data[[output_param_name]].values
x_test = test_data[[input_param_name]].values
y_test = test_data[[output_param_name]].values
```

导入后显示：

![data](./LinearRegression/img/single_data.png)

训练模型：

```python
num_iterations = 500
lr = 0.01
linear_regression = LinearRegression(x_train, y_train)
(theta, loss_history) = linear_regression.train(lr, num_iterations)
```

梯度下降：

![loss](./LinearRegression/img/single_loss.png)

测试及回归结果：

```python
preditions_num = 100
x_predictions = np.linspace(x_train.min(), x_train.max(), preditions_num).reshape(preditions_num, 1)
y_predictions = linear_regression.predict(x_predictions)
```

![result](./LinearRegression/img/single_result.png)

- #### 1.2.3 多特征建立线性回归模型

[源码](LinearRegression/MultivariateLinearRegression.ipynb)

使用的数据为[world_happiness_report2017.csv](LinearRegression/data/world_happiness_report2017.csv)

**目的**：训练出GDP，Freedom与幸福指数的线性回归模型

核心代码：

```python
linear_regression = LinearRegression(x_train, y_train, polynomial_degree, sinusoid_degree)
(theta, loss_history) = linear_regression.train(lr, num_iterations)
```

![multi_data](LinearRegression/img/multi_data.png)

![multi_loss](LinearRegression/img/multi_loss.png)

![multi_result](LinearRegression/img/multi_result.png)



- #### 1.2.4 非线性回归模型

[源码](LinearRegression/Non_linearRegression.ipynb)

使用的数据为[china_gdp.csv](LinearRegression/data/china_gdp.csv)

导入数据：

![nonlinear_data](LinearRegression/img/nonlinear_data.png)

使用 $sigmoid$ 函数进行拟合：
$$
Y=\frac{1}{1+e^{\beta_1(X-\beta_2)}}
$$


![nonlinear_result](LinearRegression/img/nonlinear_result.png)





## 第二章 模型评估方法

[notebook](ModelEvaluationMethods/model_evaluation_methods.ipynb)



- ### 2.1 交叉验证

交叉验证（循环估计），是一种统计学上将数据样本切分成较小子集的实用方法。将数据集切分为训练集和测试集(test data)，其中在训练集内再切分为训练集(train date)和验证集(validation data)。交叉验证的目的，是用未训练过的新数据测试模型的性能，以便减少诸如过拟合和选择偏差等问题。



- #### 2.1.1 LOOCV

LOOCV(Leave-one-out-cross-validation)方法是只用一个数据作为验证集，其他数据都作为训练集，并将此步骤重复N次（N为数据集的数据数量）。

假设现在有n个数据组成的数据集，那么LOOCV方法就是每次取出一个数据作为验证集的唯一元素，而其他n-1个数据都作为训练集用于训练模型和调参。结果就是我们最终训练了n个模型，每次都能得到一个MSE。
$$
CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}MSE_i
$$

- #### 2.1.2 K-fold Cross Validation

K折交叉验证，每次的验证集不再只包含一个数据，具体数目将根据k的选取决定。比如K=5：

- 将所有的训练集分成5份
- 不重复地每次取其中一份做为验证集，用其他四份做训练集训练模型，之后计算该模型在验证集上的$MSE_i$ 
- 将5次的 $MSE_i$ 取平均



![crossValidation](ModelEvaluationMethods/img/cross_validation.png)


$$
CV_{(k)}=\frac{1}{k}\sum_{i=1}^{k}MSE_i
$$


不难理解，LOOCV是一种特殊的K-fold Cross Validation(K=N)。



- #### 2.1.3 Bias-Variance Trade-Off for K-Fold Cross-Validation

对于K的选取。K越大，每次投入的训练集的数据就越多，模型的偏移越小；但是K越大，又意味着每次选取的训练集之间的相关性越大，而这种相关性会导致结果有更大的方差。一般来说，根据经验，选择K=5或者10。



- #### 2.1.4 Cross-Validation on Classification Problems

对于分类问题，可以用下式衡量：
$$
CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}Err_i
$$


其中 $Err_i$ 表示第 $i$ 个模型在第 $i$ 组验证集上的分类错误的个数。



- ### 2.2 Confusion Matrix 混淆矩阵

|         二分类问题          |      相关（Relevant），正类       |     无关（NonRelevant），负类     |
| :-------------------------: | :-------------------------------: | :-------------------------------: |
|    被检索到（Retrieved）    |  true positive(TP 正类判定为正)   | false positive(FP 负类判定为正类) |
| 未被检索到（Not Retrieved） | false negative(FN 正类判定为负类) | true negative(TN 负类判定为负类)  |



- #### 2.2.1 准确率

$$
accuracy=\frac{TP+TN}{TP+TN+FP+FN}=\frac{TP+TN}{all\ data}
$$



在数据集不平衡时，准确率将不能很好的表示模型的性能。可能会存在准确率很高，而少数类样本全分错的情况，此时应该选择其他模型评价指标。

- #### 2.2.2 Precision & Recall

$$
precision=\frac{TP}{TP+FP}
$$

表示在预测为 positive 的样本中真实类别为 true 的样本所占比例。


$$
recall=\frac{TP}{TP+FN}
$$
表示在真实类别为 true 的样本中模型预测为 positive 的样本所占比例。



- #### 2.2.3 $F_1$ score

$$
F_1=\frac{2}{\frac{1}{precision}+\frac{1}{recall}}=\frac{2\cdot precision\cdot recall}{precision+recall}
$$

$F_1$ 值就是 precision 和 recall 的调和平均值。如果召回率和查准率都很高，分类器将获得高 $F_1$ 分数。



- #### 2.2.4 ROC curves

recelver operating characteristic (ROC) 曲线是二元分类中的常用评估方法

ROC曲线的纵坐标 TPR 在数值上等于 positive class 的 recall；横坐标 FPR 在数值上等于 （1- negative class 的 recall）：
$$
TPR=\frac{TP}{TP+FN}=recall_{positive}
$$

$$
FPR = \frac{FP}{FP+TN}=\frac{FP+TN-TN}{FP+TN}\\=1-\frac{TN}{FP+TN}=1-recall_{negative}
$$



## 第三章 逻辑回归

- 目的： 经典的二分类算法
- 机器学习算法选择：先逻辑回归再用复杂的，能用简单还是用简单的
- 逻辑回归的决策边界：可以是非线性的

- #### 3.1 Sigmoid 函数

$$
g(z)=\frac{1}{1+e^{-z}}
$$

- 自变量取值为任意实数，值域[0, 1]

- 将任意的输入映射到了 [0,1] 区间。在线性回归中可以得到一个预测值，再将该值映射到 sigmoid 函数中，这样就完成了由值到概率的转换，也就是分类任务。

- 预测函数：
  $$
  h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}
  $$
  其中
  $$
  \theta_0+\theta_1x_1+,\cdots,+\theta_nx_n=\sum_{i=1}^{n}\theta_ix_i=\theta^Tx
  $$

- 分类任务：
  $$
  P(y=1|x;\theta)=h_\theta(x)
  $$

  $$
  P(y=0|x;\theta)=1-h_\theta(x)
  $$

  $\Rightarrow$  整合：
  $$
  P(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}\tag{*}
  $$
  
- 上式中对于二分类任务（0，1），整合后 y 取 0 只保留了后一项，取 1 只保留了前一项。



- ### 3.2 原理

- (*) 式的似然函数：

$$
L(\theta)=\prod_{i=1}^{m}P(y_i|x_i;\theta)=\prod_{i=1}^{m}(h_\theta(x_i))^{y_i}(1-h_\theta(x_i))^{1-y_i}
$$

- 对数似然：

$$
l(\theta)=logL(\theta)=\sum_{i=1}^{m}(y_ilogh_\theta(x_i)+(1-y_i)log(1-h_\theta(x_i)))
$$
​			目标：似然函数越大越好，应为梯度上升求最大值。

**引入 $J(\theta)=-\frac{1}{m}l(\theta)$  转换为梯度下降任务** 



求导过程：
$$
\frac{\delta}{\delta_{\theta_j}}J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y_i\frac{1}{h_\theta(x_i)}\frac{\delta}{\delta_{\theta_j}}h_\theta(x_i)-(1-y_i)\frac{1}{1-h_\theta(x_i)}\frac{\delta}{\delta_{\theta_j}}h_\theta(x_i))\\=-\frac{1}{m}\sum_{i=1}^{m}(y_i\frac{1}{g(\theta^Tx_i)}-(1-y_i)\frac{1}{1-g(\theta^Tx_i)})\frac{\delta}{\delta_{\theta_j}}g(\theta^Tx_i)\\=-\frac{1}{m}\sum_{i=1}^{m}(y_i\frac{1}{g(\theta^Tx_i)}-(1-y_i)\frac{1}{1-g(\theta^Tx_i)})g(\theta^Tx_i)(1-g(\theta^Tx_i))\frac{\delta}{\delta_{\theta_j}}\theta^Tx_i\\=-\frac{1}{m}\sum_{i=1}^{m}(y_i(1-g(\theta^Tx_i))-(1-y_i)g(\theta^Tx_i))x_i^j\\=-\frac{1}{m}(y_i-g(\theta^Tx_i))x_i^j
$$


- 参数更新：
  $$
  \theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)x_i^j
  $$

- 多分类的 softmax:
  $$
  h_\theta(x^{(i)})=\begin{bmatrix}p(y^{(i)}=1|x^{(i)};\theta)\\p(y^{(i)}=2|x^{(i)};\theta)\\\vdots\\p(y^{(i)}=k|x^{(i)};\theta)\end{bmatrix}=\frac{1}{\sum_{j=1}^{k}e^{\theta^T_jx^{(i)}}}\begin{bmatrix}e^{\theta^T_1x^{(i)}}\\e^{\theta^T_2x(i)}\\\vdots\\e^{\theta^T_kx^{(i)}}\end{bmatrix}
  $$





## 第四章 聚类算法

- **聚类概念**：
  - 无监督问题
  - 聚类：相似的东西分到一组
  - 难点：如何评估，如何调参



- ### 4.1 K-MEANS

- **K-MEANS** 算法基本概念：

  - 要得到簇的个数，需要指定**K**值

  - 质心：均值，即向量各维度取平均即可

  - 距离的度量：常用**欧氏距离**和**余弦相似度**（先标准化）

  - 优化目标:
    $$
    min\sum_{i=1}^{K}\sum_{x\in C_i} dist(c_i,x)^2
    $$

- **优势**：

  - 简单，快速， 适合常规数据集

- **劣势**：

  - K值难确定
  - 复杂度与样本呈线性关系
  - 很难发现任意形状的簇



- ### 4.2 DBSCAN

- 基本概念：（Density-Based Spatial Clustering of Applications with Noise）

  - 核心对象：若某个点的密度达到算法设定的阈值则其为核心点。（即 r 邻域内点的数量不小于 minPts）
  - $\varepsilon$ -邻域的距离阈值：设定的半径 r 
  - 直接密度可达：若某点 p 在点 q 的 r 邻域内，且 q 是核心点则 p-q 直接密度可达
  - 密度可达：若有一个点的序列 $q_0,q_1,\cdots,q_k$,对任意 $q_i-q_{i-1}$ 是直接密度可达的，则称从 $q_0$ 到 $q_k$ 密度可达，这实际上是直接密度可达的“传播”

- 工作流程：

  - 参数D：输入数据集

  - 参数$\epsilon$ ：指定半径

  - MinPts：密度阈值

    > 标记所有对象为 unvisited
    >
    > Do
    >
    > 随机选择一个 unvisited 对象 p
    >
    > 标记 p 为 visited
    >
    > if p 的 $\epsilon$ -领域至少有 MinPts 个对象
    >
    > > 创建一个新簇 C，并把 p 添加到 C
    > >
    > > 令 N 为 p 的 $\epsilon$ -领域中的对象集合
    > >
    > > For N 中每个点 $p^,$ 
    > >
    > > > If $p^,$ 是 unvisited
    > > >
    > > > > 标记 $p^,$ 为 visited
    > > > >
    > > > > If $p^,$ 的 $\epsilon$ -领域至少有 MinPts 个对象，把这些对象添加到 N
    > > > >
    > > > > If $p^` 还不是任何簇的成员，把 p^`$ 添加到C
    > > >
    > > > End for
    > > >
    > > > 输出C
    > >
    > > Else 标记 p 为噪声
    > >
    > > Until 没有标记为 unvisited 的对象

- 参数选择：

  - 半径 $\epsilon$ ，可以根据 K 距离来设定：找突变点
  - K 距离：给定数据集 $P=\left\{p(i);i=0,1,\cdots,n\right \}$   ，计算点 $P(i)$ 到集合 D 的子集 S 中所有点之间的距离，距离按照从小到大的顺序排序，$d(k)$ 就被称为 k-距离。
  - MinPts：k-距离中 k 的值，一般取得小一些，多次尝试

- 优势：

  - 不需要指定簇个数
  - 可以发现任意形状的簇
  - 擅长找到离群点（检测任务）
  - 两个参数就够了

- 劣势：

  - 高维度数据需要做降维处理
  - 参数难以选择（对结果影响非常大）
  - 需要数据削减，提高效率

![method](D:\ML\ML_Classic_Algorithms\KMEANS\img\001.png)





## 第五章 决策树

- 从根节点开始，一步步走到叶子节点（决策）
- 有监督学习 
- 所有的数据最终都会落到叶子节点，既可以做分类也可以做回归。
- 树的组成：
  - 根节点：第一个选择点
  - 非叶子节点与分支：中间过程
  - 叶子节点：最终的决策结果
- 决策树的训练与测试
  - 训练阶段：从给定的训练集构造出一个树（从根节点开始选择特征，如何进行**特征切分**
  - 测试阶段：根据构造出来的树模型从上到下去走一遍
- 如何选择节点（特征切分）：通过一种衡量标准，来计算通过不同特征进行分支选择后的分类情况，找到最好的那个，作为节点。
- 衡量标准：熵
  - 表示随机变量不确定性的度量
  - $H(x)=-\sum p(i)\cdot logp(i),i=1,2,\cdots,n$ 
  - 不确定性越大，得到的熵值越大
  - 信息增益：表示特征X使得类Y的不确定性减少的程度。（分类后的专一性，希望分类后的结果是同类在一起）
  - GINI系数：$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$ 
- 决策树剪枝策略
  - 决策树过拟合风险很大，理论上可以完全分得开数据
  - 预剪枝：边建立决策树边进行剪枝（更实用），限制深度，叶子节点个数，叶子节点样本数，信息增益量等
  - 后剪枝：当建立完决策树后来进行剪枝，通过一定的衡量标准$C_\alpha(T)=C(T)+\alpha\cdot|T_{leaf}|$ （叶子节点越多，损失越大）



## 第六章 集成算法

-  Ensemble Leaning

  - 目的：让学习效果更好

  - **Bagging**: 训练多个分类器取平均 $f(x)=\frac{1}{M}\sum_{m=1}^{M}f_m(x)$ 

  - **Boosting**: 从弱学习器开始加强，通过加权来进行训练

    $F_m(x)=F_{m-1}(x)+argmin_h\sum_{i=1}^{n}L(y_i,F_{m-1}(x_i))+h(x_i)$ (加入一颗树，要比原来强)

  - **Stacking**: 聚合多个分类或回归模型（可以分阶段来做）

- Bagging 模型

  - 全称：booststrap aggregation(并行训练一堆分类器)
  - 最典型代表：随机森林模型
  - 随机：**数据采样随机， 特征选择随机**
  - 森林：很多个决策树并行放在一起
  - 之所以进行随机，是要保证泛化能力，如果树都一样，就没有意义了

  - 随机森林优势
    - 能够处理很高维度（feature 很多）的数据，并且不用做特征选择
    - 在训练完成后，能够给出哪些 feature 比较重要（特征重要性）
    - 容易做成并行化方法，速度快
    - 可以进行可视化展示，可解释性强，便于分析
    - 理论上越多的树效果会越好，但实际上基本超过一定数量就差不多上下浮动了

- Boosting 提升算法模型

  - 典型代表：AdaBoost, Xgboost
  - Adaboost 会根据前一次的分类效果调整数据权重
  - 解释：如果某一个数据在这次分错了，那在下一次就会给他更大的权重
  - 最终结果：每个分类器根据自身的准确性来确定各自的权重，再合体

- Stacking 堆叠模型

  - 堆叠：暴力，一堆分类器
  - 可以堆叠各种分类器（KNN，SVM，RF等）
  - 分阶段：第一阶段得出各自结果，第二阶段再用前一阶段结果训练
  - 为了刷好的结果不择手段



## 第七章 支持向量机

1. 要解决的问题：什么样的决策边界才是最好的(二分类)
2. 特征数据本身就很难分，怎么办
3. 计算复杂度怎么样，能否实际应用
4. 目标：基于上述问题读SVM进行推导



- 决策边界：选出离“雷区”最远的（边界上的点，要 Large Margin）

- 距离计算：点到平面的距离
  $$
  distance(x,b,w)=|\frac{w^T}{||w||}(x-x^`)|=\frac{1}{||w||}|w^Tx+b|
  $$

- 数据标签定义

  - 数据集：$(X_1, Y_1)(X_2,Y_2)\cdots(X_n,Y_n)$ 

  - $Y$ 为样本的类别：当 $X$ 为正例的时候$Y=+1$ 当$X$ 为负例的时候$Y=-1$ 

  - 决策方程：$y(x)=w^T\Phi(x)+b$ (其中$\Phi(x)$ 是对数据做了变换)
    $$
    \Rightarrow \begin{align*}y(x_i)>0\Leftrightarrow y_i=+1\\y(x_i)<0\Leftrightarrow y_i=-1  \end{align*}\Rightarrow y_i\cdot y(x_i)>0
    $$

- 优化的目标

  - 找到一条线（w和b），使得离该线最近的点（雷区）能够最远

  - 将点到直线的距离化简得：
    $$
    \frac{y_i(w^T\cdot \Phi(x_i)+b)}{||w||}
    $$
    (由于$y_i\cdot y(x_i)>0$所以将绝对值展开原式依旧成立)

- 目标函数

  - 放缩变换：对于决策方程$(w,b)$可以通过放缩使得其结果值$|Y|\ge1$
    $$
    \Rightarrow y_i\cdot(w^T\cdot\Phi(x_i)+b)\ge1
    $$
    (之前认为恒大于0)

  - 优化目标：
    $$
    argmax_{(w,b)}\left\{\frac{1}{||w||}min_i\left[y_i\cdot(w^T\cdot\Phi(x_i)+b)\right]\right\}
    $$
    由于 $y_i\cdot(w^T\cdot\Phi(x_i)+b)\ge1$ ，只需要考虑 $argmax_{(w,b)}\frac{1}{||w||}$ 

  - 如何求解：应用**拉格朗日乘子法求解（在条件下求极值）**

- SVM求解

  - 分别对 $w$ 和 $b$ 求偏导，分别得到两个条件(对偶性质)
    $$
    min_{w,b} max_{\alpha}L(w,b,\alpha)\rightarrow max_\alpha min_{w,b}L(w,b,\alpha)
    $$

  - 对 $w$ 求偏导：$\frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{i=1}^{n}\alpha_iy_i\Phi(x_n)$

  - 对 $b$ 求偏导：$\frac{\partial L}{\partial b}=0\Rightarrow 0=\sum_{i=1}^{n}\alpha_iy_i$ 

- soft-margin

  - 软间隔：有时候数据中有一些噪音点，如果考虑它们分类会边困难
  - 松弛因子 $y_i(w\cdot x_i+b)\ge 1-\xi_i$ 
  - 新目标函数：$min\frac{1}{2}||w||^2+C\sum_{i=1}^{n}\xi_i$ 
  - 当$C$ 趋近于很大时：意味着分类严格不能有错误
  - 当 $C$ 趋近于很小时：意味着可以有更大得错误容忍
  - $C$ 时需要指定的参数（超参数）

- **低维不可分问题**

  - 核变换：低维时不可分，将数据映射到高维
  - 目标：找到一种变换方法，也就是 $\Phi(X)$ 
  - 高斯核函数：$K(X,Y)=exp\left\{-\frac{||X-Y||^2}{2\sigma^2}\right\}$ 



## 第八章 深度学习

- 学习流程
  - 数据获取
  - 特征工程（**最核心**）
    - 数据特征决定了模型的上限
    - 预处理和特征提取是最核心的
    - 算法与参数选择决定了如何逼近这个上限
  - 建立模型
  - 评估与应用

- 计算机视觉：

  - 图像分类任务

- 计算机视觉面临的挑战：

  照射角度、形状改变、部分遮蔽、背景混入



- **K近邻**
  1. 计算已知类别数据集中的点与当前点的距离
  2. 按照距离一次排序
  3. 选取与当前点距离最小的K个点
  4. 确定前K个点所在类别的出现概率
  5. 返回前K个点出现频率最高的类别作为当前点预测分类
- K近邻分析
  - KNN算法本身简单有效，它是一种 lazy-learning 算法。
  - 分类器不需要使用训练集进行训练，**训练时间复杂度**为0 。
  - KNN分类的**计算复杂度**和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为$n$，那么KNN的**分类时间复杂度**为$O(n)$。
  - **K值得选择，距离度量和分类决策规则**是该算法的三个基本要素
- 为什么K近邻不好用来做图像分类
  - 背景主导是一个最大的问题，而任务关注的是主体（主要成分）



- ### 神经网络基础

  - **线性函数**（得分函数）：从输入 $\rightarrow$ 输出的映射(10分类)
    $$
    \ce{image(32\times32\times3) ->[f(x,W)_{10\times1}=W_{10\times3070}\cdot x_{3070\times1}(+b_{10\times1})]}\ce{每个类别的得分}
    $$
    多组权重参数构成了决策边界
    $$
    f(x_i, W, b)=Wx_i+b
    $$

  - **损失函数**：结果的得分值有着明显的差异，我们需要明确得知道模型得当前效果，有多好或是多差。

    做不同的任务，就是损失函数得区别。
    $$
    损失函数 = 数据损失 + 正则化惩罚项
    $$

    $$
    L = \frac{1}{N}\sum_{i=1}^{N}\sum_{j\ne y_i}max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)+\lambda R(W)
    $$

    正则化惩罚项：$R(W)=\sum_k\sum_lW^2_{k,l}$ （我们总是希望模型不要太复杂，过拟合得模型是没有用的）

  - **Softmax**分类器

    将得到的一个输入的得分值，转换成一个概率值$g(z)=\frac{1}{1+e^{-z}}$ (sigmoid函数)

    softmax则为：
    $$
    \ce{归一化：}P(Y=k|X=x_i)=\frac{e^{s_k}}{\sum_je^{s_j}},\  \ce{where}\ (s=f(x_i;W))
    $$

    $$
    \ce{计算损失值：}L_i=-logP(Y=y_i|X=x_i)
    $$

  - **前向传播**：得出损失值

    ```mermaid
    graph LR
    x --> A((*))
    W --> A((*))
    A((*)) -- scores --> B((loss)) --> C((+)) --> L
    W --> D((R)) --> C
    ```

  - **反向传播**：更新模型（梯度下降）
  
    计算梯度时，是逐层计算。（链式法则：梯度是一步一步传的）



- ### 神经网络整体架构

  - 层次结构(Layer)，神经元(数据的量，矩阵的大小)，全连接，非线性

  - 线性方程：$f=Wx$

  - 非线性方程：$f=W_2\cdot max(0,W_1x)$ 

  - 计算结果：$\ce{x_{3072} ->[W_1]h_100->[W_2]s_10}$ 

  - 基本结构：$f=W_2\cdot NonlinearFunc(W_1x)$ 

    继续堆叠一层：$f=W_3\cdot NF(W_2\cdot NF(W_1x))$

    神经网络的强大之处在于，用更多的参数来拟合复杂的数据。

  - 正则化的作用：防止**过拟合**

  - 神经元：个数对结果的影响，数量越多，过拟合风险越大

  - **激活函数**(非常重要的一部分)：常用的激活函数(Sigmoid, Relu, Tanh 等)

  - 数据预处理：不同的预处理结果会使得模型的效果发生很大差异

  - **参数初始化**：同样非常重要，通常用随机策略来进行参数初始化`W = 0.01 * np.random.randn(D, H)` 

  - **Dropout**：过拟合是神经网络非常头疼的大问题。

    在神经网络训练的过程中，随机地杀死一部分神经元

    

    

    
