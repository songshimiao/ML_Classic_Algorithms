# 机器学习经典算法学习笔记

##  第一章 线性回归

### 原理

线性回归为**有监督算法（需要标签）**。

假设 $\theta_1$ 是 $x_1$ 的参数，$\theta_2$ 是 $x_2$ 的参数

拟合的平面： $h_\theta(x)=\theta_0+\theta_{1} x_1+\theta_2 x_2$ 

**整合**：
$$
h_\theta(x)=\sum_{i=0}^{n}\theta_i x_i=\theta^Tx
$$

**误差**：

真实值和预测值之间 存在的差异（$\varepsilon$)

对于每个样本：
$$
y^{(i)}=\theta^Tx^{(i)}+\varepsilon^{(i)}\tag{1}
$$
​                                                  

**误差 $\varepsilon^{(i)}$  是独立同分布，是服从均值为 0 方差为 $\theta^2$ 的高斯分布 **（机器学习的基础）

由于误差服从高斯分布：
$$
p(\varepsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\varepsilon^{(i)})^2}{2\sigma^2})\tag{2}
$$
将（1）式带入（2）式：
$$
p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
$$
求得 $(x,\theta)$ 与 $y$ 越接近越好！

**似然函数**：
$$
L(\theta) = \prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta) = \prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
$$
**对数似然**：
$$
log\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
$$
展开化简：
$$
\sum_{i=1}^{m}log\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})
= m\cdot log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2
$$
目标：是让似然函数越大越好

最小二乘法：
$$
J(\theta) = \frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2
$$
目标：让$J(\theta)$ 越小越好！（目标函数，损失函数 Loss Function）

**目标函数求解**

目标函数：
$$
J(\theta) = \frac{1}{2}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 = \frac{1}{2}(X\theta-y)^T(X\theta-y)
$$
求偏导：
$$
\bigtriangledown_\theta J(\theta)=\bigtriangledown_\theta(\frac{1}{2}(X\theta-y)^T(X\theta-y)=\bigtriangledown_\theta(\frac{1}{2}(\theta^TX^T-y^T)(X\theta-y))=\bigtriangledown_\theta(\frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty))=\frac{1}{2}(2X^TX\theta-X^Ty-(y^TX)^T)=X^TX\theta-X^Ty
$$
偏导等于0：
$$
\theta = (X^TX)^{-1}X^Ty
$$

**梯度下降**

寻找目标函数的“终点”（什么样的参数能使目标函数达到极值点） 

目标函数：
$$
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(y^i-h_\theta(x^i))^2
$$
**批量梯度下降**：
$$
\frac{\partial J(\theta)}{\partial \theta_j} = -\frac{1}{m}\sum_{i=1}^{m}(y^i-h_\theta(x^i))x_j^i\theta_j^\prime = \theta_j+\frac{1}{m}\sum_{i=1}^{m}(y^i-h_\theta(x^i))x_j^i
$$
(容易得到最优解，但是由于每次考虑所以样本，速度很慢)

**随机梯度下降(SGD)**：
$$
\theta_j^\prime = \theta_j + (y^i-h_\theta(x^i))x_j^i
$$
（每次找一个样本，迭代速度快，但不一定每次都朝着收敛方向）

**小批量梯度下降法（mini batch）**：
$$
\theta_j := \theta_j - \alpha\frac{1}{10}\sum_{k=i}^{i+9}(h_\theta(x^{(k)})-y^{(k)})x_j^{(k)}
$$
(每次更新选择一小部分数据来算，实用！)

**学习率（步长）learning rate**：对结果产生巨大的影响，一般小一些

### 代码实现

#### 模块编写

**归一化模块**

[normalize.py](LinearRegression/normalize.py)

计算特征（输入）的均值feature_mean，标准差feature_deviation。

归一化操作：
$$
features\_normalized = \frac{x-\mu}{\sigma}
$$
返回归一化后的特征，均值，标准差

**训练前要做的准备**

[prepare_for_traning.py](LinearRegression/prepare_for_training.py)

- 计算样本总数
- 预处理（调用normalize模块）
- 返回处理后的数据，均值，标准差

**定义线性回归类**

[linear_regression.py](LinearRegression/linear_regression.py)

#### 单变量线性回归实现

[源码](LinearRegression/UnivariateLinearRegression.ipynb)

使用的数据为[world_happiness_report2017.csv](LinearRegression/data/world_happiness_report2017.csv)

**目的**：训练出GDP与幸福指数之间的线性回归方程

导入数据：

```python
data = pd.read_csv('data/world_happiness_report2017.csv')
train_data = data.sample(frac=0.8)
test_data = data.drop(train_data.index)
input_param_name = 'Economy..GDP.per.Capita.'
output_param_name = 'Happiness.Score'
x_train = train_data[[input_param_name]].values
y_train = train_data[[output_param_name]].values
x_test = test_data[[input_param_name]].values
y_test = test_data[[output_param_name]].values
```

导入后显示：

![data](./LinearRegression/img/single_data.png)

训练模型：

```python
num_iterations = 500
lr = 0.01
linear_regression = LinearRegression(x_train, y_train)
(theta, loss_history) = linear_regression.train(lr, num_iterations)
```

梯度下降：

![loss](./LinearRegression/img/single_loss.png)

测试及回归结果：

```python
preditions_num = 100
x_predictions = np.linspace(x_train.min(), x_train.max(), preditions_num).reshape(preditions_num, 1)
y_predictions = linear_regression.predict(x_predictions)
```

![result](./LinearRegression/img/single_result.png)

#### 多特征建立线性回归模型

[源码](LinearRegression/MultivariateLinearRegression.ipynb)

使用的数据为[world_happiness_report2017.csv](LinearRegression/data/world_happiness_report2017.csv)

**目的**：训练出GDP，Freedom与幸福指数的线性回归模型

核心代码：

```python
linear_regression = LinearRegression(x_train, y_train, polynomial_degree, sinusoid_degree)
(theta, loss_history) = linear_regression.train(lr, num_iterations)
```

![multi_data](LinearRegression/img/multi_data.png)

![multi_loss](LinearRegression/img/multi_loss.png)

![multi_result](LinearRegression/img/multi_result.png)



